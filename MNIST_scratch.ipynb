{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c9d6ec5",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; margin-bottom:20px;\">\n",
    "Forward and Backward propagation of a 2-layer neural network\n",
    "</h1>\n",
    "\n",
    "\n",
    "## Network Definition\n",
    "$$\n",
    "z_1 = XW_1 + b_1,\\quad a_1 = \\mathrm{ReLU}(z_1)\n",
    "$$\n",
    "$$\n",
    "z_2 = a_1W_2 + b_2,\\quad \\hat{Y} = \\mathrm{softmax}(z_2)\n",
    "$$\n",
    "$$\n",
    "\\mathcal{L} = -\\frac{1}{B}\\sum_{i=1}^B \\log(\\hat{y}_{i,y_i})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Objective\n",
    "Compute gradients:\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_2},\\;\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_2},\\;\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1},\\;\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Step 1 — Softmax + Cross-Entropy\n",
    "\n",
    "Softmax:\n",
    "$$\n",
    "\\hat{y}_k = \\frac{e^{z_k}}{\\sum_j e^{z_j}}\n",
    "$$\n",
    "\n",
    "Cross-Entropy:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_k y_k \\log \\hat{y}_k\n",
    "$$\n",
    "\n",
    "Gradient simplification:\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_2}\n",
    "=\n",
    "\\frac{1}{B}(\\hat{Y} - Y)\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "## Step 2 — Output Linear Layer\n",
    "$$\n",
    "z_2 = a_1W_2 + b_2\n",
    "$$\n",
    "\n",
    "Weight gradient:\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_2}\n",
    "=\n",
    "a_1^T \\frac{\\partial \\mathcal{L}}{\\partial z_2}\n",
    "}\n",
    "$$\n",
    "\n",
    "Bias gradient:\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_2}\n",
    "=\n",
    "\\sum_{i=1}^B \\frac{\\partial \\mathcal{L}}{\\partial z_2^{(i)}}\n",
    "}\n",
    "$$\n",
    "\n",
    "Hidden activation gradient:\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a_1}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_2} W_2^T\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Step 3 — ReLU Nonlinearity\n",
    "$$\n",
    "a_1 = \\max(0, z_1)\n",
    "$$\n",
    "\n",
    "Derivative:\n",
    "$$\n",
    "\\mathrm{ReLU}'(z_1) =\n",
    "\\begin{cases}\n",
    "1 & z_1 > 0 \\\\\n",
    "0 & z_1 \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Chain rule:\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial z_1}\n",
    "=\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial a_1}\n",
    "\\odot \\mathbb{1}(z_1 > 0)\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Step 4 — First Linear Layer\n",
    "$$\n",
    "z_1 = XW_1 + b_1\n",
    "$$\n",
    "\n",
    "Weight gradient:\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1}\n",
    "=\n",
    "X^T \\frac{\\partial \\mathcal{L}}{\\partial z_1}\n",
    "}\n",
    "$$\n",
    "\n",
    "Bias gradient:\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b_1}\n",
    "=\n",
    "\\sum_{i=1}^B \\frac{\\partial \\mathcal{L}}{\\partial z_1^{(i)}}\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Final Collapsed Gradient Chain\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1}\n",
    "=\n",
    "X^T\n",
    "\\Big[\n",
    "\\big((\\hat{Y} - Y) W_2^T\\big)\n",
    "\\odot \\mathbb{1}(z_1 > 0)\n",
    "\\Big]\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_2}\n",
    "=\n",
    "a_1^T (\\hat{Y} - Y)\n",
    "}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668202f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST data\n",
    "print(\"Loading MNIST...\")\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, cache=True)\n",
    "\n",
    "# Normalize and prepare data\n",
    "X = X.astype(np.float32) / 255.0\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=60000, test_size=10000, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize network weights\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(784, 128) / np.sqrt(784)\n",
    "b1 = np.zeros((1, 128))\n",
    "W2 = np.random.randn(128, 10) / np.sqrt(128)\n",
    "b2 = np.zeros((1, 10))\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.1\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Activation functions\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    e = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return e / np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle data\n",
    "    idx = np.random.permutation(len(X_train))\n",
    "    X_shuf, y_shuf = X_train[idx], y_train[idx]\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        Xb = X_shuf[i:i+batch_size]\n",
    "        yb = y_shuf[i:i+batch_size]\n",
    "        B = len(Xb)\n",
    "        \n",
    "        # Forward pass\n",
    "        z1 = Xb @ W1 + b1\n",
    "        a1 = relu(z1)\n",
    "        z2 = a1 @ W2 + b2\n",
    "        probs = softmax(z2)\n",
    "        \n",
    "        # Compute loss\n",
    "        log_probs = np.log(probs[range(B), yb] + 1e-10)\n",
    "        loss = -np.mean(log_probs)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        dz2 = probs.copy()\n",
    "        dz2[range(B), yb] -= 1\n",
    "        dz2 /= B\n",
    "        \n",
    "        dW2 = a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        da1 = dz2 @ W2.T\n",
    "        dz1 = da1 * (z1 > 0)\n",
    "        \n",
    "        dW1 = Xb.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "    \n",
    "    print(f\"Epoch {epoch+1:2d}/{epochs}   loss = {np.mean(losses):.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "z1 = X_test @ W1 + b1\n",
    "a1 = relu(z1)\n",
    "z2 = a1 @ W2 + b2\n",
    "preds = np.argmax(softmax(z2), axis=1)\n",
    "\n",
    "accuracy = (preds == y_test).mean() * 100\n",
    "print(f\"\\nTest accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1280320",
   "metadata": {},
   "source": [
    "$$X∈RB×784$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
