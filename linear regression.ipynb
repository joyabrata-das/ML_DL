{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091ee2f9",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Linear Regression</h1>\n",
    "\n",
    "This code creates synthetic data for a simple linear regression problem. It generates evenly spaced input values and computes the corresponding output using a linear relationship defined by a slope and an intercept. Random noise is added to the output to simulate real-world data, where observations are not perfectly linear. This synthetic dataset can be used to demonstrate and test linear regression algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad9f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x= np.linspace(0,10,50)\n",
    "true_w=2.5\n",
    "true_b=4\n",
    "y=true_w*x+true_b+np.random.randn(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5979a4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This function computes the **cost** of a linear regression model using the **Mean Squared Error (MSE)** criterion. The cost measures how far the modelâ€™s predictions are from the actual target values.\n",
    "\n",
    "For a linear model defined as:\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "the cost function is given by:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ m $ is the number of training examples  \n",
    "- $\\hat{y}^{(i)}$ is the predicted value  \n",
    "- $y^{(i)}$ is the actual value  \n",
    "\n",
    "The function iterates over all data points, computes the prediction using the current weight and bias, calculates the squared error for each example, and then averages the total error. A lower cost value indicates a better fit of the model to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ce4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost function\n",
    "def compute_cost(x,y,w,b):\n",
    "    m=len(x)\n",
    "    total_error=0\n",
    "    for i in range(m):\n",
    "        prediction= w*x[i]+b\n",
    "        error= prediction-y[i]\n",
    "        total_error+=error**2\n",
    "    cost=total_error/(2*m)\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cfa27",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This function computes the **gradients** of the cost function with respect to the model parameters **weight** and **bias**. These gradients indicate how the parameters should be adjusted to reduce the prediction error during gradient descent.\n",
    "\n",
    "The linear regression model is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "The cost function is:\n",
    "\n",
    "$$\n",
    "J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "The gradients of the cost function are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{y}^{(i)} - y^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "The function iterates over all training examples, computes the prediction error for each example, accumulates the contributions to the gradients, and then averages them. These gradient values are used by gradient descent to update the model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a477cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad(x,y,w,b):\n",
    "    m=len(x)\n",
    "    dw=0\n",
    "    db=0\n",
    "    for i in range(m):\n",
    "        prediction=w*x[i]+b\n",
    "        error= prediction-y[i]\n",
    "        dw+=error*x[i]\n",
    "        db+=error\n",
    "    dw=dw/m\n",
    "    db=db/m\n",
    "    return dw,db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3d80d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This function implements **gradient descent** to learn the optimal values of the weight and bias for a linear regression model. Gradient descent is an iterative optimization algorithm that minimizes the cost function by updating parameters step by step.\n",
    "\n",
    "The linear regression model is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = wx + b\n",
    "$$\n",
    "\n",
    "At each iteration, the parameters are updated using the gradient descent update rules:\n",
    "\n",
    "$$\n",
    "w := w - \\alpha \\frac{\\partial J}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial J}{\\partial b}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the learning rate  \n",
    "- $\\frac{\\partial J}{\\partial w}$ and $\\frac{\\partial J}{\\partial b}$ are the gradients of the cost function  \n",
    "\n",
    "The algorithm starts with initial values of the weight and bias set to zero. In each iteration, it computes the gradients, updates the parameters, and gradually reduces the cost. The cost value is periodically printed to monitor convergence. After completing all iterations, the function returns the learned weight and bias values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932d0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,learning_rate,iter):\n",
    "    w=0\n",
    "    b=0\n",
    "    for i in range(iter):\n",
    "        dw,db=compute_grad(x,y,w,b)\n",
    "        w=w-learning_rate*dw\n",
    "        b=b-learning_rate*db\n",
    "        if i%100==0:\n",
    "            cost=compute_cost(x,y,w,b)\n",
    "            print(f\"iteration:{i} Cost={cost:.4f},w={w:.4f},b={b:.4f}\")\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e0a92",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This step runs the gradient descent algorithm on the given dataset to learn the optimal values of the weight and bias for the linear regression model. The learning rate controls the step size of each update, while the number of iterations determines how many times the model parameters are adjusted.\n",
    "\n",
    "After training is complete, the final learned values of the weight and bias are printed. These values represent the parameters of the best-fit line that minimizes the cost function for the given data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f090a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:0 Cost=68.7726,w=1.0264,b=0.1608\n",
      "iteration:100 Cost=0.9520,w=2.8906,b=1.0768\n",
      "iteration:200 Cost=0.7114,w=2.8182,b=1.5610\n",
      "iteration:300 Cost=0.5661,w=2.7619,b=1.9372\n",
      "iteration:400 Cost=0.4783,w=2.7182,b=2.2296\n",
      "iteration:500 Cost=0.4254,w=2.6842,b=2.4567\n",
      "iteration:600 Cost=0.3934,w=2.6578,b=2.6332\n",
      "iteration:700 Cost=0.3741,w=2.6373,b=2.7704\n",
      "iteration:800 Cost=0.3624,w=2.6213,b=2.8769\n",
      "iteration:900 Cost=0.3554,w=2.6089,b=2.9597\n",
      "iteration:1000 Cost=0.3511,w=2.5993,b=3.0241\n",
      "iteration:1100 Cost=0.3486,w=2.5918,b=3.0741\n",
      "iteration:1200 Cost=0.3470,w=2.5860,b=3.1129\n",
      "iteration:1300 Cost=0.3461,w=2.5815,b=3.1431\n",
      "iteration:1400 Cost=0.3455,w=2.5780,b=3.1666\n",
      "iteration:1500 Cost=0.3452,w=2.5753,b=3.1848\n",
      "iteration:1600 Cost=0.3450,w=2.5732,b=3.1989\n",
      "iteration:1700 Cost=0.3448,w=2.5715,b=3.2099\n",
      "iteration:1800 Cost=0.3448,w=2.5702,b=3.2185\n",
      "iteration:1900 Cost=0.3447,w=2.5692,b=3.2251\n",
      "final learned parameters:\n",
      "w = 2.5684692026438594\n",
      "b =  3.230242045849406\n"
     ]
    }
   ],
   "source": [
    "final_w,final_b= gradient_descent(x,y,learning_rate=0.01,iter=2000)\n",
    "print(\"final learned parameters:\")\n",
    "print(\"w =\",final_w)\n",
    "print(\"b = \",final_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
